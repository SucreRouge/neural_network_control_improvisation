{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=cpu,floatX=float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/__init__.py:1357: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "import glob2 as glob\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sbn\n",
    "import deepdish as dd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "import neural_networks as nn\n",
    "from music_utils import generateSequence\n",
    "from nnet_utils import get_next_batch_rnn\n",
    "%matplotlib inline\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data from each class\n",
    "datapath = '/Users/rafaelvalle/Desktop/datasets/Piano/'\n",
    "glob_folder = '/Users/rafaelvalle/Desktop/datasets/Piano/*/'\n",
    "glob_file = '*.npy'\n",
    "data_dict = defaultdict(list)\n",
    "n_pieces = 40\n",
    "for folderpath in glob.glob(glob_folder):\n",
    "    composer = os.path.basename(os.path.normpath(folderpath))\n",
    "    filepaths = glob.glob(os.path.join(os.path.join(datapath, composer), '*.npy'))\n",
    "    for filepath in np.random.choice(filepaths, n_pieces):\n",
    "        data_dict[composer].append(np.load(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_batch_size = 512\n",
    "g_batch_size = 512\n",
    "min_len = 32\n",
    "max_len = 64\n",
    "n_features = data_dict[data_dict.keys()[0]][0].shape[1]\n",
    "n_conditions = len(data_dict.keys())\n",
    "n_labels = n_conditions + 1\n",
    "temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_nonlinearity(data, temperature=1):\n",
    "    return T.clip(lasagne.nonlinearities.softmax(lasagne.nonlinearities.linear(data / temperature)), 1e-7, 1 - 1e-7)\n",
    "    \n",
    "softmax_temperature = functools.partial(output_nonlinearity, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_specs = {'batch_size': d_batch_size,\n",
    "           'input_shape': (None, None, n_features),\n",
    "           'mask_shape': (None, None),\n",
    "           'n_output_units': n_labels,\n",
    "           'n_units': 32,\n",
    "           'n_hidden': 32,\n",
    "           'grad_clip': 100.,\n",
    "           'init': lasagne.init.HeUniform(),\n",
    "           'non_linearities': (\n",
    "              lasagne.nonlinearities.tanh,  # feedforward\n",
    "              lasagne.nonlinearities.tanh,  # feedbackward\n",
    "              softmax_temperature),  # apply sotfmax with temperature\n",
    "           'learning_rate': 0.1,           \n",
    "          }\n",
    "\n",
    "g_specs = {'batch_size': g_batch_size,\n",
    "           'input_shape': (None, None, n_features),\n",
    "           'noise_shape': (None, None, n_features),\n",
    "           'cond_shape': (None, None, n_conditions),\n",
    "           'mask_shape': (None, None),\n",
    "           'n_output_units': n_features,\n",
    "           'n_units': 32,\n",
    "           'n_hidden': 32 ,\n",
    "           'grad_clip': 100.,\n",
    "           'init': lasagne.init.HeUniform(),\n",
    "           'non_linearities': (\n",
    "              lasagne.nonlinearities.tanh,  # feedforward\n",
    "              lasagne.nonlinearities.tanh,  # feedbackward\n",
    "              lasagne.nonlinearities.rectify),  # apply sotfmax with temperature\n",
    "           'learning_rate': 0.1,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare theano variables\n",
    "d_in_X = T.ftensor3('ddata')\n",
    "d_in_M = T.imatrix('dismask')\n",
    "g_in_D = T.ftensor3('gdata')\n",
    "g_in_Z = T.ftensor3('noise')\n",
    "g_in_C = T.ftensor3('condition')\n",
    "g_in_M = T.imatrix('genmask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gate_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), \n",
    "    W_hid=lasagne.init.Orthogonal(),\n",
    "    b=lasagne.init.Constant(0.),\n",
    "    nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "cell_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), \n",
    "    W_hid=lasagne.init.Orthogonal(),\n",
    "    W_cell=None, b=lasagne.init.Constant(0.),\n",
    "    nonlinearity=lasagne.nonlinearities.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_discriminator(params):\n",
    "    # input layers\n",
    "    l_in = lasagne.layers.InputLayer(shape=d_specs['input_shape'], name='d_in') \n",
    "    l_mask = lasagne.layers.InputLayer(shape=d_specs['mask_shape'], name='d_mask')\n",
    "\n",
    "    # recurrent layers for bidirectional network\n",
    "    l_forward = lasagne.layers.RecurrentLayer(\n",
    "        l_in, d_specs['n_units'], grad_clipping=d_specs['grad_clip'],\n",
    "        W_in_to_hid=d_specs['init'], W_hid_to_hid=d_specs['init'],\n",
    "        nonlinearity=d_specs['non_linearities'][0], only_return_final=True, mask_input=l_mask)\n",
    "    l_backward = lasagne.layers.RecurrentLayer(\n",
    "        l_in, d_specs['n_units'], grad_clipping=d_specs['grad_clip'],\n",
    "        W_in_to_hid=d_specs['init'], W_hid_to_hid=d_specs['init'],        \n",
    "        nonlinearity=d_specs['non_linearities'][1], only_return_final=True, mask_input=l_mask,\n",
    "        backwards=True)\n",
    "\n",
    "    # concatenate output of forward and backward layers\n",
    "    l_concat = lasagne.layers.ConcatLayer([l_forward, l_backward])\n",
    "\n",
    "    # output layer\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_concat, num_units=d_specs['n_output_units'], nonlinearity=d_specs['non_linearities'][2])\n",
    "\n",
    "    class Discriminator:\n",
    "        def __init__(self, l_in, l_mask, l_out):\n",
    "            self.l_in = l_in\n",
    "            self.l_mask = l_mask\n",
    "            self.l_out = l_out\n",
    "            \n",
    "    return Discriminator(l_in, l_mask, l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_generator_lstm(params):\n",
    "    # input layers\n",
    "    l_in = lasagne.layers.InputLayer(shape=g_specs['input_shape'], input_var=g_in_D, name='g_in')\n",
    "    l_noise = lasagne.layers.InputLayer(shape=g_specs['noise_shape'], input_var=g_in_Z, name='g_noise')\n",
    "    l_cond = lasagne.layers.InputLayer(shape=g_specs['cond_shape'], input_var=g_in_C, name='g_cond')\n",
    "    l_mask = lasagne.layers.InputLayer(shape=g_specs['mask_shape'], input_var=g_in_M, name='g_mask')\n",
    "    \n",
    "    # recurrent layers for bidirectional network\n",
    "    l_forward_data = lasagne.layers.recurrent.LSTMLayer(\n",
    "        l_in, g_specs['n_units'], mask_input=l_mask, \n",
    "        ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "        cell=cell_parameters, outgate=gate_parameters,\n",
    "        learn_init=True, grad_clipping=g_specs['grad_clip'], only_return_final=True)\n",
    "    l_forward_noise = lasagne.layers.recurrent.LSTMLayer(\n",
    "        l_noise, g_specs['n_units'], mask_input=l_mask, \n",
    "        ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "        cell=cell_parameters, outgate=gate_parameters,\n",
    "        learn_init=True, grad_clipping=g_specs['grad_clip'], only_return_final=True)\n",
    "    l_forward_cond = lasagne.layers.recurrent.LSTMLayer(\n",
    "        l_cond, g_specs['n_units'], mask_input=l_mask, \n",
    "        ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "        cell=cell_parameters, outgate=gate_parameters,\n",
    "        learn_init=True, grad_clipping=g_specs['grad_clip'], only_return_final=True)\n",
    "\n",
    "    l_backward_data = lasagne.layers.recurrent.LSTMLayer(\n",
    "        l_in, g_specs['n_units'], mask_input=l_mask, \n",
    "        ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "        cell=cell_parameters, outgate=gate_parameters,\n",
    "        learn_init=True, grad_clipping=g_specs['grad_clip'], only_return_final=True,\n",
    "        backwards=True)\n",
    "    l_backward_noise = lasagne.layers.recurrent.LSTMLayer(\n",
    "        l_noise, g_specs['n_units'], mask_input=l_mask, \n",
    "        ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "        cell=cell_parameters, outgate=gate_parameters,\n",
    "        learn_init=True, grad_clipping=g_specs['grad_clip'], only_return_final=True,\n",
    "        backwards=True)\n",
    "    l_backward_cond = lasagne.layers.recurrent.LSTMLayer(\n",
    "        l_cond, g_specs['n_units'], mask_input=l_mask, \n",
    "        ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "        cell=cell_parameters, outgate=gate_parameters,\n",
    "        learn_init=True, grad_clipping=g_specs['grad_clip'], only_return_final=True,\n",
    "        backwards=True)\n",
    "\n",
    "    # sum linearities of data and condition on forward and and backward recurrent layers\n",
    "    l_forward_sum = lasagne.layers.ElemwiseSumLayer([l_forward_data, l_forward_noise, l_forward_cond])\n",
    "    l_backward_sum = lasagne.layers.ElemwiseSumLayer([l_backward_data, l_backward_noise, l_backward_cond])\n",
    "\n",
    "    # apply nonlinearity to forward and backward recurrent layers\n",
    "    l_forward_nonlinearity = lasagne.layers.NonlinearityLayer(l_forward_sum, \n",
    "        nonlinearity=g_specs['non_linearities'][0])\n",
    "    l_backward_nonlinearity = lasagne.layers.NonlinearityLayer(l_backward_sum, \n",
    "        nonlinearity=g_specs['non_linearities'][1])\n",
    "\n",
    "    # concatenate output of forward and backward layers\n",
    "    l_concat = lasagne.layers.ConcatLayer(\n",
    "        [l_forward_nonlinearity, l_backward_nonlinearity])\n",
    "\n",
    "    # output layer where time is collapsed into one dimension\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_concat, num_units=g_specs['n_output_units'], nonlinearity=g_specs['non_linearities'][2])        \n",
    "\n",
    "    class Generator:\n",
    "        def __init__(self, l_in, l_noise, l_cond, l_mask, l_out):\n",
    "            self.l_in = l_in\n",
    "            self.l_noise = l_noise\n",
    "            self.l_cond = l_cond\n",
    "            self.l_mask = l_mask\n",
    "            self.l_out = l_out\n",
    "\n",
    "    return Generator(l_in, l_noise, l_cond, l_mask, l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_generator(params):\n",
    "    # input layers\n",
    "    l_in = lasagne.layers.InputLayer(shape=g_specs['input_shape'], input_var=g_in_D, name='g_in')\n",
    "    l_noise = lasagne.layers.InputLayer(shape=g_specs['noise_shape'], input_var=g_in_Z, name='g_noise')\n",
    "    l_cond = lasagne.layers.InputLayer(shape=g_specs['cond_shape'], input_var=g_in_C, name='g_cond')\n",
    "    l_mask = lasagne.layers.InputLayer(shape=g_specs['mask_shape'], input_var=g_in_M, name='g_mask')\n",
    "\n",
    "    # recurrent layers for bidirectional network\n",
    "    l_forward_data = lasagne.layers.RecurrentLayer(\n",
    "        l_in, g_specs['n_units'], grad_clipping=g_specs['grad_clip'],\n",
    "        W_in_to_hid=g_specs['init'], W_hid_to_hid=g_specs['init'],\n",
    "        nonlinearity=lasagne.nonlinearities.linear, only_return_final=True, mask_input=l_mask)\n",
    "    l_forward_noise = lasagne.layers.RecurrentLayer(\n",
    "        l_noise, g_specs['n_units'], b=lasagne.init.Constant(0.), grad_clipping=g_specs['grad_clip'], \n",
    "        W_in_to_hid=g_specs['init'], W_hid_to_hid=g_specs['init'],        \n",
    "        nonlinearity=lasagne.nonlinearities.linear, only_return_final=True, mask_input=l_mask)\n",
    "    l_forward_cond = lasagne.layers.RecurrentLayer(\n",
    "        l_cond, g_specs['n_units'], b=lasagne.init.Constant(0.), grad_clipping=g_specs['grad_clip'], \n",
    "        W_in_to_hid=g_specs['init'], W_hid_to_hid=g_specs['init'],        \n",
    "        nonlinearity=lasagne.nonlinearities.linear, only_return_final=True, mask_input=l_mask)\n",
    "    l_backward_data = lasagne.layers.RecurrentLayer(\n",
    "        l_in, g_specs['n_units'], grad_clipping=g_specs['grad_clip'],\n",
    "        W_in_to_hid=g_specs['init'], W_hid_to_hid=g_specs['init'],        \n",
    "        nonlinearity=lasagne.nonlinearities.linear, only_return_final=True,\n",
    "        backwards=True, mask_input=l_mask)\n",
    "    l_backward_noise = lasagne.layers.RecurrentLayer(\n",
    "        l_noise, g_specs['n_units'], b=lasagne.init.Constant(0.), grad_clipping=g_specs['grad_clip'],\n",
    "        W_in_to_hid=g_specs['init'], W_hid_to_hid=g_specs['init'],        \n",
    "        nonlinearity=lasagne.nonlinearities.linear, only_return_final=True,\n",
    "        backwards=True, mask_input=l_mask)    \n",
    "    l_backward_cond = lasagne.layers.RecurrentLayer(\n",
    "        l_cond, g_specs['n_units'], b=lasagne.init.Constant(0.), grad_clipping=g_specs['grad_clip'],\n",
    "        W_in_to_hid=g_specs['init'], W_hid_to_hid=g_specs['init'],        \n",
    "        nonlinearity=lasagne.nonlinearities.linear, only_return_final=True,\n",
    "        backwards=True, mask_input=l_mask)\n",
    "\n",
    "    # sum linearities of data and condition on forward and and backward recurrent layers\n",
    "    l_forward_sum = lasagne.layers.ElemwiseSumLayer([l_forward_data, l_forward_noise, l_forward_cond])\n",
    "    l_backward_sum = lasagne.layers.ElemwiseSumLayer([l_backward_data, l_backward_noise, l_backward_cond])\n",
    "\n",
    "    # apply nonlinearity to forward and backward recurrent layers\n",
    "    l_forward_nonlinearity = lasagne.layers.NonlinearityLayer(l_forward_sum, \n",
    "        nonlinearity=g_specs['non_linearities'][0])\n",
    "    l_backward_nonlinearity = lasagne.layers.NonlinearityLayer(l_backward_sum, \n",
    "        nonlinearity=g_specs['non_linearities'][1])\n",
    "\n",
    "    # concatenate output of forward and backward layers\n",
    "    l_concat = lasagne.layers.ConcatLayer(\n",
    "        [l_forward_nonlinearity, l_backward_nonlinearity])\n",
    "\n",
    "    # output layer where time is collapsed into one dimension\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_concat, num_units=g_specs['n_output_units'], nonlinearity=g_specs['non_linearities'][2])    \n",
    "\n",
    "\n",
    "    class Generator:\n",
    "        def __init__(self, l_in, l_noise, l_cond, l_mask, l_out):\n",
    "            self.l_in = l_in\n",
    "            self.l_noise = l_noise\n",
    "            self.l_cond = l_cond\n",
    "            self.l_mask = l_mask\n",
    "            self.l_out = l_out\n",
    "\n",
    "    return Generator(l_in, l_noise, l_cond, l_mask, l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_training(discriminator, generator, d_specs, g_specs):\n",
    "    # get variables from discrimiator and generator\n",
    "    d_params = lasagne.layers.get_all_params(discriminator.l_out, trainable=True)\n",
    "    g_params = lasagne.layers.get_all_params(generator.l_out, trainable=True)\n",
    "    \n",
    "    d_labels = T.fmatrix('d_label')\n",
    "    g_labels = T.fmatrix('g_label')    \n",
    "\n",
    "    # G(z)\n",
    "    g_z = lasagne.layers.get_output(generator.l_out,\n",
    "                                    inputs={generator.l_in: g_in_D,\n",
    "                                            generator.l_noise: g_in_Z,\n",
    "                                            generator.l_cond: g_in_C,\n",
    "                                            generator.l_mask: g_in_M})\n",
    "\n",
    "    # create proper G(z) mask\n",
    "    g_in_M_idx = g_in_M.sum(axis=1)\n",
    "    g_in_M_alt = theano.clone(g_in_M) \n",
    "    g_in_M_alt = T.set_subtensor(g_in_M_alt[T.arange(g_in_M_alt.shape[0]), g_in_M_idx], \n",
    "                                 T.ones_like(g_in_M_alt[:, 0]))\n",
    "\n",
    "    # create proper G(z)\n",
    "    g_in_D_alt = theano.clone(g_in_D)\n",
    "    g_in_D_alt = T.set_subtensor(g_in_D_alt[T.arange(g_in_D_alt.shape[0]), g_in_M_idx], \n",
    "                                 g_z)\n",
    "\n",
    "    # D(G(z))\n",
    "    d_g_z = lasagne.layers.get_output(\n",
    "        discriminator.l_out,         \n",
    "        inputs={discriminator.l_in: g_in_D_alt, \n",
    "                discriminator.l_mask: g_in_M_alt})\n",
    "\n",
    "    g_loss = lasagne.objectives.categorical_crossentropy(1 - d_g_z, g_labels)        \n",
    "    g_loss = g_loss.mean()\n",
    "\n",
    "    g_updates = lasagne.updates.adagrad(g_loss, g_params, g_specs['learning_rate'])\n",
    "    g_train_fn = theano.function(inputs=[g_in_D, g_in_Z, g_in_C, g_in_M, g_labels],\n",
    "                                 outputs=g_loss, \n",
    "                                 updates=g_updates)\n",
    "    g_sample_fn = theano.function(inputs=[g_in_D, g_in_Z, g_in_C, g_in_M],\n",
    "                                  outputs=g_z)\n",
    "\n",
    "    # D(x)\n",
    "    d_x = lasagne.layers.get_output(discriminator.l_out,\n",
    "                                    inputs={discriminator.l_in: d_in_X, \n",
    "                                            discriminator.l_mask: d_in_M})\n",
    "  \n",
    "    d_x_loss = lasagne.objectives.categorical_crossentropy(d_x, d_labels)\n",
    "    d_g_z_loss = lasagne.objectives.categorical_crossentropy(d_g_z, g_labels)\n",
    "    d_loss = d_x_loss + d_g_z_loss\n",
    "    d_loss = d_loss.mean()    \n",
    "    \n",
    "    d_updates = lasagne.updates.adagrad(d_loss, d_params, d_specs['learning_rate'])\n",
    "    d_train_fn = theano.function(\n",
    "        inputs=[d_in_X, d_in_M, g_in_D, g_in_Z, g_in_C, g_in_M, d_labels, g_labels],\n",
    "        outputs=[d_loss, d_x_loss, d_g_z_loss, d_x, d_g_z, g_in_D_alt], \n",
    "        updates=d_updates) \n",
    "\n",
    "    d_predict_fn = None\n",
    "\n",
    "#   d_predict_fn = theano.function(\n",
    "#        inputs=[d_in_X, d_in_M],\n",
    "#        outputs=lasagne.layers.get_output(discriminator[2],\n",
    "#                                          inputs={discriminator[0]: d_in_X, \n",
    "#                                                  discriminator[1]: d_in_M}))\n",
    "\n",
    "    return d_train_fn, d_predict_fn, g_train_fn, g_sample_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_generator_batch(batch_size, inputs, conds, masks):\n",
    "    excerpt = np.random.permutation(len(inputs))[:batch_size]\n",
    "    # create random noise\n",
    "    noises = np.random.normal(size=inputs[excerpt].shape).astype('float32') \n",
    "    # create labels\n",
    "    lbls = np.zeros((batch_size, conds.shape[2]), dtype=np.float32)\n",
    "    lbls[:, n_conditions-1] = 1\n",
    "    \n",
    "    return inputs[excerpt], noises, conds[excerpt], masks[excerpt], lbls\n",
    "\n",
    "\n",
    "def sample_data(data, batch_size, min_len, max_len, clip=False):\n",
    "    encoding = defaultdict(lambda _: 0)\n",
    "    i = 0\n",
    "    for k in data.keys():\n",
    "        encoding[k] = i\n",
    "        i += 1        \n",
    "    \n",
    "    pieces_per_lbl = int(batch_size / len(data.keys()))\n",
    "\n",
    "    while True:\n",
    "        inputs, targets, labels, conds, masks = [], [], [], [], []\n",
    "        for k, lbl in encoding.items():\n",
    "                pieces = np.random.choice(data[k], pieces_per_lbl)\n",
    "                for piece in pieces:\n",
    "                    start_idx = np.random.randint(0, piece.shape[0] - max_len - 1)\n",
    "                    piece_data = piece[start_idx: start_idx+max_len]\n",
    "                    mask_size = np.random.randint(min_len, max_len)\n",
    "                    if clip:\n",
    "                        piece_data[mask_size:] = 0\n",
    "                    target = piece[start_idx+max_len]\n",
    "                    label = np.zeros(len(encoding)+1, dtype=np.float32) #+1 for generator label\n",
    "                    label[encoding[k]] = 1\n",
    "\n",
    "                    cond = np.zeros((max_len, len(encoding)), dtype=np.float32) #+1 for generator label\n",
    "                    cond[:, encoding[k]] = 1                \n",
    "\n",
    "                    mask = np.zeros(max_len, dtype=np.int32)\n",
    "                    mask[:mask_size] = 1            \n",
    "\n",
    "                    inputs.append(piece_data)\n",
    "                    targets.append(target)\n",
    "                    labels.append(label)\n",
    "                    conds.append(cond)\n",
    "                    masks.append(mask)\n",
    "                \n",
    "        inputs = np.array(inputs, dtype=np.float32)\n",
    "        targets = np.array(targets, dtype=np.float32)\n",
    "        labels = np.array(labels, dtype=np.float32)\n",
    "        conds = np.array(conds, dtype=np.float32)        \n",
    "        masks = np.array(masks, dtype=np.int32)\n",
    "        \n",
    "        shuffle_ids = np.random.randint(0, len(inputs), len(inputs))\n",
    "        yield inputs[shuffle_ids], targets[shuffle_ids], labels[shuffle_ids], conds[shuffle_ids], masks[shuffle_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discriminator = build_discriminator(d_specs)\n",
    "generator = build_generator_lstm(g_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_train_fn, d_predict_fn, g_train_fn, g_sample_fn = build_training(discriminator, generator, d_specs, g_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_iter = sample_data(data_dict, d_batch_size, min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre training\n",
    "n_d_iterations_pre = 0\n",
    "d_losses_pre = []\n",
    "\n",
    "folderpath = 'piano_gan_pre_{}_dlr_{}_glr_{}_bs_{}_temp_{}'.format(\n",
    "    n_d_iterations_pre, d_specs['learning_rate'], g_specs['learning_rate'], d_batch_size, temperature)\n",
    "\n",
    "if not os.path.exists(os.path.join('images', folderpath)):\n",
    "    os.makedirs(os.path.join('images', folderpath))\n",
    "\n",
    "for i in range(n_d_iterations_pre):\n",
    "    # use same data for discriminator and generator\n",
    "    d_X, _, d_L, d_C, d_M = data_iter.next()\n",
    "    g_Z = np.random.normal(size=d_X.shape).astype('float32')     \n",
    "    g_L = np.zeros((g_Z.shape[0], n_labels), dtype=np.float32)\n",
    "    g_L[:, -1] = 1\n",
    "    g_D, g_C, g_M = d_X, d_C, d_M\n",
    "    d_loss, d_x_loss, d_g_z_loss, d_x, d_g_z, g_in_D_alt = d_train_fn(d_X, d_M, g_D, g_Z, g_C, g_M, d_L, g_L)\n",
    "    d_losses_pre.append(d_loss)\n",
    "    if i == (n_d_iterations_pre -1):\n",
    "        fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "        axes[0, 0].set_title('D Label')\n",
    "        sbn.heatmap(d_L.T, ax=axes[0, 0])\n",
    "        axes[0, 1].set_title('G Label')\n",
    "        sbn.heatmap(g_L.T, ax=axes[0, 1])\n",
    "\n",
    "        axes[1, 0].set_title('D(x)')\n",
    "        sbn.heatmap(d_x.T, ax=axes[1, 0])\n",
    "        axes[1, 1].set_title('D(G(z))')        \n",
    "        sbn.heatmap(d_g_z.T, ax=axes[1, 1])\n",
    "\n",
    "        axes[2, 0].set_title('G(z) : 0')\n",
    "        axes[2, 1].set_title('G(z) : 1')\n",
    "        axes[3, 0].set_title('G(z) : 2')\n",
    "        axes[3, 1].set_title('G(z) : 3')        \n",
    "        sbn.heatmap(g_in_D_alt[0].T, ax=axes[2, 0]).invert_yaxis()\n",
    "        sbn.heatmap(g_in_D_alt[1].T, ax=axes[2, 1]).invert_yaxis()\n",
    "        sbn.heatmap(g_in_D_alt[2].T, ax=axes[3, 0]).invert_yaxis()\n",
    "        sbn.heatmap(g_in_D_alt[3].T, ax=axes[3, 1]).invert_yaxis()        \n",
    "\n",
    "        axes[4, 0].set_title('Loss(d)')                        \n",
    "        axes[4, 0].plot(d_losses_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [6:16:42<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "## store losses\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "n_iterations = int(1e4)\n",
    "n_d_iterations = 1\n",
    "n_g_iterations = 1\n",
    "epoch = 100\n",
    "for iteration in tqdm(range(n_iterations)):\n",
    "    for i in range(n_d_iterations):\n",
    "        d_X, d_T, d_L, d_C, d_M = data_iter.next()\n",
    "        g_Z = np.random.normal(size=d_X.shape).astype('float32')\n",
    "        g_L = np.zeros((g_Z.shape[0], n_labels), dtype=np.float32)\n",
    "        g_L[:, -1] = 1\n",
    "        g_D, g_C, g_M = d_X, d_C, d_M\n",
    "        d_loss, d_x_loss, d_g_z_loss, d_x, d_g_z, g_in_D_alt = d_train_fn(d_X, d_M, g_D, g_Z, g_C, g_M, d_L, g_L)\n",
    "        d_losses.append(d_loss)\n",
    "        \n",
    "    for i in range(n_g_iterations):\n",
    "        d_X, d_T, d_L, d_C, d_M = data_iter.next()\n",
    "        g_Z = np.random.normal(size=d_X.shape).astype('float32')     \n",
    "        g_D, g_C, g_M = d_X, d_C, d_M\n",
    "        \n",
    "        g_loss = g_train_fn(g_D, g_Z, g_C, g_M, g_L)\n",
    "        g_losses.append(g_loss)\n",
    "\n",
    "    if iteration % epoch == 0:\n",
    "        fig, axes = plt.subplots(5, 2, figsize=(32, 32))\n",
    "        axes[0, 0].set_title('D Label')\n",
    "        sbn.heatmap(d_L.T, ax=axes[0, 0])\n",
    "        axes[0, 1].set_title('G Label')\n",
    "        sbn.heatmap(g_L.T, ax=axes[0, 1])\n",
    "        \n",
    "        axes[1, 0].set_title('D(x)')\n",
    "        sbn.heatmap(d_x.T, ax=axes[1, 0])\n",
    "        axes[1, 1].set_title('D(G(z))')        \n",
    "        sbn.heatmap(d_g_z.T, ax=axes[1, 1])\n",
    "        \n",
    "        axes[2, 0].set_title('G(z) : 0')\n",
    "        axes[2, 1].set_title('G(z) : 1')\n",
    "        axes[3, 0].set_title('G(z) : 2')\n",
    "        axes[3, 1].set_title('G(z) : 3')        \n",
    "        sbn.heatmap(g_in_D_alt[0].T, ax=axes[2, 0]).invert_yaxis()\n",
    "        sbn.heatmap(g_in_D_alt[1].T, ax=axes[2, 1]).invert_yaxis()\n",
    "        sbn.heatmap(g_in_D_alt[2].T, ax=axes[3, 0]).invert_yaxis()\n",
    "        sbn.heatmap(g_in_D_alt[3].T, ax=axes[3, 1]).invert_yaxis()        \n",
    "        \n",
    "        axes[4, 0].set_title('Loss(d)')                        \n",
    "        axes[4, 0].plot(d_losses)\n",
    "        axes[4, 1].set_title('Loss(g)')                        \n",
    "        axes[4, 1].plot(g_losses)\n",
    "        \n",
    "        fig.savefig('images/{}/iteration_{}'.format(folderpath, iteration))\n",
    "        plt.close('all') \n",
    "    \n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
