{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"device=cpu,floatX=float32,exception_verbosity=high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/__init__.py:1357: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import functools\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sbn\n",
    "import deepdish as dd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "import neural_networks as nn\n",
    "from music_utils import generateSequence\n",
    "from nnet_utils import get_next_batch_rnn\n",
    "%matplotlib inline\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate data for interval major and minor    \n",
    "int_maj = [0, 2, 2, 1, 2, 2, 2]\n",
    "int_min = [0, 2, 1, 2, 2, 1, 2]\n",
    "min_len = 2\n",
    "max_len = len(int_maj)\n",
    "input_maj, target_maj, masks_maj = generateSequence(int_maj, min_len, max_len)\n",
    "input_min, target_min, masks_min = generateSequence(int_min, min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_batch_size = 64\n",
    "g_batch_size = 64\n",
    "epoch_size = len(input_maj)\n",
    "n_timesteps = 7\n",
    "n_features = 12\n",
    "n_conditions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_data = np.concatenate((input_maj, input_min)).astype(np.float32)\n",
    "target_data = np.concatenate((target_maj, target_min)).astype(np.float32)\n",
    "masks_data = np.concatenate((masks_maj, masks_min)).astype(np.int32)\n",
    "cond_data = np.concatenate((np.zeros((len(masks_maj), n_features, n_conditions), dtype=np.float32),\n",
    "                            np.ones((len(masks_min), n_features, n_conditions), dtype=np.float32)))\n",
    "lbls_data = np.zeros((len(cond_data), n_conditions), dtype=np.float32)\n",
    "lbls_data[:len(masks_maj), 0] = 1                            \n",
    "lbls_data[len(masks_maj):, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_nonlinearity(data, temperature=1):\n",
    "    return lasagne.nonlinearities.softmax(lasagne.nonlinearities.linear(data / temperature))\n",
    "    \n",
    "nn_output_nonlinearity = functools.partial(output_nonlinearity, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_specs = {'batch_size': d_batch_size,\n",
    "           'epoch_size': epoch_size,\n",
    "           'input_shape': (None, n_timesteps, n_features),\n",
    "           'mask_shape': (None, n_timesteps),\n",
    "           'n_output_units': n_conditions,\n",
    "           'n_units': 16,\n",
    "           'n_hidden': 16,\n",
    "           'grad_clip': 100.,\n",
    "           'init': lasagne.init.HeUniform(),\n",
    "           'non_linearities': (\n",
    "              lasagne.nonlinearities.tanh,  # feedforward\n",
    "              lasagne.nonlinearities.tanh,  # feedbackward\n",
    "              nn_output_nonlinearity),  # apply sotfmax with temperature\n",
    "           'learning_rate': 1e-2,           \n",
    "          }\n",
    "\n",
    "g_specs = {'batch_size': g_batch_size,\n",
    "           'epoch_size': epoch_size,\n",
    "           'noise_shape': (None, n_timesteps, n_features),\n",
    "           'cond_shape': (None, n_timesteps, n_conditions),\n",
    "           'mask_shape': (None, n_timesteps),\n",
    "           'softmax_shape': (g_batch_size * n_timesteps, n_features),\n",
    "           'output_shape': (g_batch_size, n_timesteps, n_features),\n",
    "           'n_output_units': n_timesteps * n_features,\n",
    "           'n_units': 16,\n",
    "           'n_hidden': 16,\n",
    "           'grad_clip': 100.,\n",
    "           'init': lasagne.init.HeUniform(),\n",
    "           'non_linearities': (\n",
    "              lasagne.nonlinearities.tanh,  # feedforward\n",
    "              lasagne.nonlinearities.tanh,  # feedbackward\n",
    "              lasagne.nonlinearities.linear),  # apply sotfmax with temperature\n",
    "           'learning_rate': 1e-2,           \n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare theano variables\n",
    "d_in_X = T.ftensor3('data')\n",
    "g_in_Z = T.ftensor3('noise')\n",
    "g_in_C = T.ftensor3('condition')\n",
    "d_in_M = T.imatrix('dismask')\n",
    "g_in_M = T.imatrix('genmask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_discriminator(params):\n",
    "    # input layers\n",
    "    l_in = lasagne.layers.InputLayer(shape=d_specs['input_shape'], name='d_in') \n",
    "    l_mask = lasagne.layers.InputLayer(shape=d_specs['mask_shape'], name='d_mask')\n",
    "\n",
    "    # recurrent layers for bidirectional network\n",
    "    l_forward = lasagne.layers.RecurrentLayer(\n",
    "        l_in, d_specs['n_units'], grad_clipping=d_specs['grad_clip'],\n",
    "        W_in_to_hid=d_specs['init'], W_hid_to_hid=d_specs['init'],\n",
    "        nonlinearity=d_specs['non_linearities'][0], only_return_final=True, mask_input=l_mask)\n",
    "    l_backward = lasagne.layers.RecurrentLayer(\n",
    "        l_in, d_specs['n_units'], grad_clipping=d_specs['grad_clip'],\n",
    "        W_in_to_hid=d_specs['init'], W_hid_to_hid=d_specs['init'],        \n",
    "        nonlinearity=d_specs['non_linearities'][1], only_return_final=True, mask_input=l_mask,\n",
    "        backwards=True)\n",
    "\n",
    "    # concatenate output of forward and backward layers\n",
    "    l_concat = lasagne.layers.ConcatLayer([l_forward, l_backward])\n",
    "\n",
    "    # output layer\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_concat, num_units=d_specs['n_output_units'], nonlinearity=d_specs['non_linearities'][2])\n",
    "\n",
    "    return l_in, l_mask, l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_generator(params):\n",
    "    # input layers\n",
    "    l_input = lasagne.layers.InputLayer(shape=g_specs['noise_shape'], input_var=g_in_Z, name='g_noise')\n",
    "    l_cond = lasagne.layers.InputLayer(shape=g_specs['cond_shape'], input_var=g_in_C, name='g_cond')\n",
    "    l_mask = lasagne.layers.InputLayer(shape=g_specs['mask_shape'], input_var=g_in_M, name='g_mask')\n",
    "\n",
    "    # recurrent layers for bidirectional network\n",
    "    l_forward_data = lasagne.layers.RecurrentLayer(\n",
    "        l_input, g_specs['n_units'], grad_clipping=g_specs['grad_clip'],\n",
    "        W_in_to_hid=g_specs['init'], W_hid_to_hid=g_specs['init'],\n",
    "        nonlinearity=lasagne.nonlinearities.linear, only_return_final=True, mask_input=l_mask)\n",
    "    l_forward_cond = lasagne.layers.RecurrentLayer(\n",
    "        l_cond, g_specs['n_units'], b=None, grad_clipping=g_specs['grad_clip'], \n",
    "        W_in_to_hid=g_specs['init'], W_hid_to_hid=g_specs['init'],        \n",
    "        nonlinearity=lasagne.nonlinearities.linear, only_return_final=True, mask_input=l_mask)\n",
    "    l_backward_data = lasagne.layers.RecurrentLayer(\n",
    "        l_input, g_specs['n_units'], grad_clipping=g_specs['grad_clip'],\n",
    "        W_in_to_hid=g_specs['init'], W_hid_to_hid=g_specs['init'],        \n",
    "        nonlinearity=lasagne.nonlinearities.linear, only_return_final=True,\n",
    "        backwards=True, mask_input=l_mask)\n",
    "    l_backward_cond = lasagne.layers.RecurrentLayer(\n",
    "        l_cond, g_specs['n_units'], b=None, grad_clipping=g_specs['grad_clip'],\n",
    "        W_in_to_hid=g_specs['init'], W_hid_to_hid=g_specs['init'],        \n",
    "        nonlinearity=lasagne.nonlinearities.linear, only_return_final=True,\n",
    "        backwards=True, mask_input=l_mask)\n",
    "\n",
    "    # sum linearities of data and condition on forward and and backward recurrent layers\n",
    "    l_forward_sum = lasagne.layers.ElemwiseSumLayer([l_forward_data, l_forward_cond])\n",
    "    l_backward_sum = lasagne.layers.ElemwiseSumLayer([l_backward_data, l_backward_cond])\n",
    "\n",
    "    # apply rectify nonlinearity to forward and backward recurrent layers\n",
    "    l_forward_nonlinearity = lasagne.layers.NonlinearityLayer(l_forward_sum, \n",
    "        nonlinearity=g_specs['non_linearities'][0])\n",
    "    l_backward_nonlinearity = lasagne.layers.NonlinearityLayer(l_backward_sum, \n",
    "        nonlinearity=g_specs['non_linearities'][1])\n",
    "\n",
    "    # concatenate output of forward and backward layers\n",
    "    l_concat = lasagne.layers.ConcatLayer(\n",
    "        [l_forward_nonlinearity, l_backward_nonlinearity])\n",
    "\n",
    "    # output layer where time is collapsed into one dimension\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_concat, num_units=g_specs['n_output_units'], nonlinearity=g_specs['non_linearities'][2])    \n",
    "\n",
    "    # reshape to match discriminator's input\n",
    "    l_out_reshape = lasagne.layers.ReshapeLayer(l_out, g_specs['softmax_shape'])\n",
    "    \n",
    "    # add custom softmax\n",
    "    l_out_softmax = lasagne.layers.NonlinearityLayer(l_out_reshape, lasagne.nonlinearities.softmax) \n",
    "    \n",
    "    # reshape again to match discriminator's input, just in case it fixes the error\n",
    "    l_out_softmax_reshape = lasagne.layers.ReshapeLayer(l_out_softmax, g_specs['output_shape'])\n",
    "\n",
    "    return l_input, l_mask, l_out_softmax_reshape, l_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_training(discriminator, generator, d_specs, g_specs):\n",
    "    # get variables from discrimiator and generator\n",
    "    d_target = T.fmatrix('d_target')\n",
    "    g_target = T.fmatrix('g_target')\n",
    "    \n",
    "    d_params = lasagne.layers.get_all_params(discriminator[2], trainable=True)\n",
    "    g_params = lasagne.layers.get_all_params(generator[2], trainable=True)\n",
    "    \n",
    "    # G(z)\n",
    "    g_z = lasagne.layers.get_output(generator[2],\n",
    "                                    inputs={generator[0]:g_in_Z,\n",
    "                                            generator[1]:g_in_M,\n",
    "                                            generator[3]:g_in_C})                           \n",
    "    # D(G(z))\n",
    "    d_g_z = lasagne.layers.get_output(discriminator[2],         \n",
    "                                      inputs={discriminator[0]: g_z, \n",
    "                                              discriminator[1]: g_in_M})\n",
    "    \n",
    "    # D(x)\n",
    "    d_x = lasagne.layers.get_output(discriminator[2],\n",
    "                                    inputs={discriminator[0]: d_in_X, \n",
    "                                            discriminator[1]: d_in_M})\n",
    "\n",
    "    g_loss = lasagne.objectives.categorical_crossentropy(1 - d_g_z, g_target)\n",
    "    g_loss = g_loss.mean()\n",
    "    g_updates = lasagne.updates.adagrad(g_loss, g_params, g_specs['learning_rate'])\n",
    "    g_train_fn = theano.function(\n",
    "        inputs=[g_in_Z, g_in_M, g_in_C, g_target],\n",
    "        outputs=g_loss, updates=g_updates)\n",
    "    g_sample_fn = theano.function(\n",
    "        inputs=[g_in_Z, g_in_M, g_in_C],\n",
    "        outputs=lasagne.layers.get_output(generator[2]))\n",
    "\n",
    "    d_loss = (lasagne.objectives.categorical_crossentropy(d_x, d_target) + \n",
    "              lasagne.objectives.categorical_crossentropy(d_g_z, g_target))\n",
    "    d_loss = d_loss.mean()        \n",
    "    d_updates = lasagne.updates.adagrad(d_loss, d_params, d_specs['learning_rate'])\n",
    "    d_train_fn = theano.function(\n",
    "        inputs=[d_in_X, d_in_M, g_in_Z, g_in_M, g_in_C, d_target, g_target],\n",
    "        outputs=[d_loss, d_x, g_z, d_g_z, d_target, g_target], updates=d_updates) \n",
    "    d_predict_fn = None\n",
    "#   d_predict_fn = theano.function(\n",
    "#        inputs=[d_in_X, d_in_M],\n",
    "#        outputs=lasagne.layers.get_output(discriminator[2],\n",
    "#                                          inputs={discriminator[0]: d_in_X, \n",
    "#                                                  discriminator[1]: d_in_M}))\n",
    "\n",
    "    return d_train_fn, d_predict_fn, g_train_fn, g_sample_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_noise_batch(batch_size, min_len, max_len, n_timesteps, n_features, n_conditions):\n",
    "    noises = np.random.normal(size=(batch_size, n_timesteps, n_features)).astype('float32') \n",
    "    \n",
    "    # one hot for condition\n",
    "    conditions = np.zeros((batch_size, n_timesteps, n_conditions), dtype=np.float32)\n",
    "    conditions[:, :, 2] = 1\n",
    "\n",
    "    # create random masks\n",
    "    masks = np.zeros((batch_size, max_len), dtype=np.int32)\n",
    "    r_ints = np.random.randint(min_len, max_len, batch_size)\n",
    "    for i in xrange(batch_size):\n",
    "        masks[i, :r_ints[i]] = 1\n",
    "\n",
    "    # create labels\n",
    "    lbls = np.zeros((batch_size, n_conditions), dtype=np.float32)\n",
    "    lbls[:, n_conditions-1] = 1\n",
    "        \n",
    "    return noises, masks, conditions, lbls\n",
    "\n",
    "\n",
    "def sample_data_batch(batch_size, inputs, targets, masks, lbls, conds=None):\n",
    "    excerpt = np.random.permutation(len(inputs))[:batch_size]\n",
    "    if conds is None:\n",
    "        return inputs[excerpt], targets[excerpt], masks[excerpt], lbls[excerpt]\n",
    "    else:\n",
    "        return inputs[excerpt], targets[excerpt], masks[excerpt], conds[excerpt], lbls[excerpt], \n",
    "    \n",
    "    \n",
    "def sample_melodies(nrow, ncol, batch_size, min_len, max_len, n_timesteps, n_features, n_conditions):\n",
    "    g_Z, g_M, g_C, _ = sample_noise_batch(batch_size, min_len, max_len, n_timesteps, n_features, n_conditions)\n",
    "    melodies = g_sample_fn(g_Z, g_M, g_C)\n",
    "    ids = np.random.randint(0, len(melodies), batch_size)\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    for i in range(nrow * ncol):\n",
    "        plt.subplot(nrow, ncol, i+1)\n",
    "        sbn.heatmap(melodies[ids[i]].T, annot=True).invert_yaxis()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def sample_probas(batch_size, inputs, targets, masks):\n",
    "    plt.hist(d_predict(sample_data_batch(batch_size)).ravel(), \n",
    "             label='D(x)', alpha=0.5)\n",
    "\n",
    "    noises, _, conditions = sample_noise_batch(100, min_len, max_len, n_timesteps, n_features, n_conditions)    \n",
    "    plt.hist(d_predict(g_output_fn(noises, masks, conditions)), \n",
    "             label='D(G(z))',alpha=0.5)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlim(0,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discriminator = build_discriminator(d_specs)\n",
    "generator = build_generator(g_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_train_fn, d_predict_fn, g_train_fn, g_sample_fn = build_training(discriminator, generator, d_specs, g_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [49:27<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# store losses\n",
    "d_losses = [0]\n",
    "g_losses = [0]\n",
    "\n",
    "n_epochs = 2000\n",
    "n_d_epochs = 1\n",
    "n_g_epochs = 1\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for i in range(n_d_epochs):\n",
    "        d_X, _,  d_M, d_L = sample_data_batch(d_specs['batch_size'], input_data,target_data, masks_data, lbls_data)\n",
    "        g_Z, g_M, g_C, g_L = sample_noise_batch(g_specs['batch_size'], min_len, max_len, n_timesteps, n_features, n_conditions)\n",
    "        d_loss, d_x, g_z, d_g_z, d_label, g_label = d_train_fn(d_X, d_M, g_Z, g_M, g_C, d_L, g_L)        \n",
    "        d_losses.append((d_losses[-1] + float(d_loss)) / float(len(d_losses)))\n",
    "\n",
    "    for i in range(n_g_epochs):\n",
    "        g_Z, g_M, g_C, g_L = sample_noise_batch(g_specs['batch_size'], min_len, max_len, n_timesteps, n_features, n_conditions)\n",
    "        g_loss = g_train_fn(g_Z, g_M, g_C, g_L)\n",
    "        g_losses.append((g_losses[-1] + float(g_loss)) / float(len(g_losses)))        \n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "        axes[0, 0].set_title('D(x)')\n",
    "        sbn.heatmap(d_x.T, ax=axes[0, 0])\n",
    "        axes[1, 0].set_title('D label')        \n",
    "        sbn.heatmap(d_label.T, ax=axes[1, 0])\n",
    "        axes[0, 1].set_title('D(G(z))')        \n",
    "        sbn.heatmap(d_g_z.T, ax=axes[0, 1])\n",
    "        axes[1, 1].set_title('G label')                \n",
    "        sbn.heatmap(g_label.T, ax=axes[1, 1])\n",
    "        axes[2, 0].set_title('Loss(d)')                        \n",
    "        axes[2, 0].plot(d_losses)\n",
    "        axes[2, 1].set_title('Loss(g)')                        \n",
    "        axes[2, 1].plot(g_losses)      \n",
    "        fig.savefig('images/epoch_{}'.format(epoch))\n",
    "        plt.close('all') \n",
    "    \n",
    "        display.clear_output(wait=True)\n",
    "        fig = sample_melodies(2, 2, g_specs['batch_size'], min_len, max_len, n_timesteps, n_features, n_conditions)\n",
    "        fig.savefig('images/samples_{}'.format(epoch))\n",
    "        plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
